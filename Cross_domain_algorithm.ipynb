{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbZXdsAfsu0DY2a7R/K4WD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash1629/cross-domain/blob/main/Cross_domain_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppu5OulLIPIc",
        "outputId": "8e9f63a2-a70e-4434-d5c1-629e7920e1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Domain Connection Algorithm Demo\n",
            "==================================================\n",
            "Loaded 20 concepts for analysis...\n",
            "Vector space built successfully!\n",
            "\n",
            "Analyzing connection between 'intuition' and 'consciousness'\n",
            "--------------------------------------------------\n",
            "Direct similarity: 0.067\n",
            "Connection path: intuition -> system_1 -> consciousness\n",
            "\n",
            "Top 5 Bridge Concepts:\n",
            "1. system 1: 0.400\n",
            "2. metacognition: 0.337\n",
            "3. fringe consciousness: 0.303\n",
            "4. embodied cognition: 0.273\n",
            "5. global workspace: 0.249\n",
            "\n",
            "Detailed Analysis of Strongest Bridge: 'system 1'\n",
            "  Similarity to intuition: 0.529\n",
            "  Similarity to consciousness: 0.058\n",
            "  Novelty score: 0.991\n",
            "  Overall bridge strength: 0.400\n",
            "\n",
            "Additional Domain Pair Analysis:\n",
            "------------------------------\n",
            "\n",
            "creativity <-> memory (direct similarity: 0.008):\n",
            "  1. executive function: 0.260\n",
            "\n",
            "emotion <-> decision making (direct similarity: 0.014):\n",
            "  1. system 1: 0.301\n",
            "  2. embodied cognition: 0.249\n",
            "  3. intuition: 0.245\n",
            "\n",
            "attention <-> learning (direct similarity: 0.033):\n",
            "  1. executive function: 0.291\n",
            "  2. global workspace: 0.278\n",
            "  3. metacognition: 0.262\n",
            "\n",
            "Sample Similarity Matrix (for debugging):\n",
            "----------------------------------------\n",
            "intuition <-> consciousness: 0.067\n",
            "intuition <-> metacognition: 0.163\n",
            "intuition <-> system_1: 0.529\n",
            "intuition <-> fringe_consciousness: 0.143\n",
            "consciousness <-> metacognition: 0.305\n",
            "consciousness <-> system_1: 0.058\n",
            "consciousness <-> fringe_consciousness: 0.155\n",
            "metacognition <-> system_1: 0.125\n",
            "metacognition <-> fringe_consciousness: 0.149\n",
            "system_1 <-> fringe_consciousness: 0.060\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import math\n",
        "from typing import Dict, List, Tuple, Set\n",
        "import requests\n",
        "import time\n",
        "\n",
        "class SemanticVectorSpace:\n",
        "    \"\"\"Handles semantic vector representation of concepts\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.concept_vectors = {}\n",
        "        self.vectorizer = TfidfVectorizer(max_features=300, stop_words='english')\n",
        "        self.concept_descriptions = {}\n",
        "\n",
        "    def add_concept(self, concept: str, description: str = None):\n",
        "        \"\"\"Add a concept with optional description\"\"\"\n",
        "        self.concept_descriptions[concept] = description or concept\n",
        "\n",
        "    def build_vector_space(self, concepts_with_descriptions: Dict[str, str] = None):\n",
        "        \"\"\"Build vector space from concept descriptions\"\"\"\n",
        "        if concepts_with_descriptions:\n",
        "            self.concept_descriptions.update(concepts_with_descriptions)\n",
        "\n",
        "        descriptions = list(self.concept_descriptions.values())\n",
        "        concept_names = list(self.concept_descriptions.keys())\n",
        "\n",
        "        if len(descriptions) < 2:\n",
        "            raise ValueError(\"Need at least 2 concepts to build vector space\")\n",
        "\n",
        "        # Create TF-IDF vectors\n",
        "        tfidf_matrix = self.vectorizer.fit_transform(descriptions)\n",
        "\n",
        "        # Store vectors for each concept\n",
        "        for i, concept in enumerate(concept_names):\n",
        "            self.concept_vectors[concept] = tfidf_matrix[i].toarray().flatten()\n",
        "\n",
        "    def similarity(self, concept1: str, concept2: str) -> float:\n",
        "        \"\"\"Calculate cosine similarity between two concepts\"\"\"\n",
        "        if concept1 not in self.concept_vectors or concept2 not in self.concept_vectors:\n",
        "            return 0.0\n",
        "\n",
        "        vec1 = self.concept_vectors[concept1]\n",
        "        vec2 = self.concept_vectors[concept2]\n",
        "\n",
        "        # Handle zero vectors\n",
        "        if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return 1 - cosine(vec1, vec2)\n",
        "\n",
        "    def get_neighborhood(self, concept: str, threshold: float = 0.65) -> Set[str]:\n",
        "        \"\"\"Get concepts similar to the given concept above threshold\"\"\"\n",
        "        if concept not in self.concept_vectors:\n",
        "            return set()\n",
        "\n",
        "        neighborhood = set()\n",
        "        for other_concept in self.concept_vectors:\n",
        "            if other_concept != concept:\n",
        "                sim = self.similarity(concept, other_concept)\n",
        "                if sim > threshold:\n",
        "                    neighborhood.add(other_concept)\n",
        "\n",
        "        return neighborhood\n",
        "\n",
        "class CitationDataAnalyzer:\n",
        "    \"\"\"Analyzes citation patterns and calculates novelty scores\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.citation_data = {}\n",
        "        self.concept_cooccurrence = defaultdict(int)\n",
        "        self.max_frequency = 1\n",
        "\n",
        "    def add_citation_data(self, concept_frequencies: Dict[str, int]):\n",
        "        \"\"\"Add citation frequency data for concepts\"\"\"\n",
        "        self.citation_data.update(concept_frequencies)\n",
        "        if concept_frequencies:\n",
        "            self.max_frequency = max(max(concept_frequencies.values()), self.max_frequency)\n",
        "\n",
        "    def calculate_novelty(self, concept: str) -> float:\n",
        "        \"\"\"Calculate novelty score based on inverse citation frequency\"\"\"\n",
        "        citation_count = self.citation_data.get(concept, self.max_frequency / 100)\n",
        "\n",
        "        # Avoid division by zero\n",
        "        if self.max_frequency == 0:\n",
        "            return 0.5\n",
        "\n",
        "        novelty = 1.0 - (citation_count / self.max_frequency)\n",
        "        # Apply logarithmic scaling to smooth the distribution\n",
        "        novelty = 0.5 + 0.5 * (np.log1p(novelty * 9) / np.log(10))\n",
        "        return max(0.0, min(1.0, novelty))\n",
        "\n",
        "    def calculate_connection_novelty(self, concept1: str, concept2: str) -> float:\n",
        "        \"\"\"Calculate how novel a connection between two concepts is\"\"\"\n",
        "        connection_key = tuple(sorted([concept1, concept2]))\n",
        "        cooccurrence = self.concept_cooccurrence.get(connection_key, 0)\n",
        "\n",
        "        # Higher novelty for less co-occurring concepts\n",
        "        max_cooccurrence = max(self.concept_cooccurrence.values()) if self.concept_cooccurrence else 1\n",
        "        novelty = 1.0 - (cooccurrence / max_cooccurrence)\n",
        "        return max(0.0, min(1.0, novelty))\n",
        "\n",
        "    def add_cooccurrence_data(self, cooccurrence_data: Dict[Tuple[str, str], int]):\n",
        "        \"\"\"Add concept co-occurrence data\"\"\"\n",
        "        self.concept_cooccurrence.update(cooccurrence_data)\n",
        "\n",
        "class CrossDomainConnector:\n",
        "    \"\"\"Main algorithm for discovering cross-domain connections\"\"\"\n",
        "\n",
        "    def __init__(self, vector_space: SemanticVectorSpace, citation_analyzer: CitationDataAnalyzer,\n",
        "                 alpha: float = 0.35, beta: float = 0.35, gamma: float = 0.2,\n",
        "                 delta: float = 0.1, threshold: float = 0.65):\n",
        "        \"\"\"\n",
        "        Initialize the cross-domain connector\n",
        "\n",
        "        Args:\n",
        "            vector_space: Semantic vector space for concepts\n",
        "            citation_analyzer: Citation data analyzer\n",
        "            alpha: Weight for similarity to domain 1\n",
        "            beta: Weight for similarity to domain 2\n",
        "            gamma: Weight for novelty\n",
        "            delta: Weight for commonality penalty\n",
        "            threshold: Similarity threshold for neighborhoods\n",
        "        \"\"\"\n",
        "        self.vector_space = vector_space\n",
        "        self.citation_analyzer = citation_analyzer\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.delta = delta\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def commonality(self, bridge: str, domain1: str, domain2: str) -> float:\n",
        "        \"\"\"Calculate how common the bridge concept is (penalty factor)\"\"\"\n",
        "        # Use citation frequency as proxy for commonality\n",
        "        citation_count = self.citation_analyzer.citation_data.get(bridge, 0)\n",
        "        max_citations = self.citation_analyzer.max_frequency\n",
        "\n",
        "        if max_citations == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return citation_count / max_citations\n",
        "\n",
        "    def bridge_strength(self, bridge: str, domain1: str, domain2: str) -> float:\n",
        "        \"\"\"Calculate the strength of a bridge between two domains\"\"\"\n",
        "        sim1 = self.vector_space.similarity(bridge, domain1)\n",
        "        sim2 = self.vector_space.similarity(bridge, domain2)\n",
        "        novelty = self.citation_analyzer.calculate_novelty(bridge)\n",
        "        commonality = self.commonality(bridge, domain1, domain2)\n",
        "\n",
        "        strength = (self.alpha * sim1 +\n",
        "                   self.beta * sim2 +\n",
        "                   self.gamma * novelty -\n",
        "                   self.delta * commonality)\n",
        "\n",
        "        return max(0.0, strength)\n",
        "\n",
        "    def identify_bridges(self, domain1: str, domain2: str, top_n: int = 10) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Identify top bridges between two domains\"\"\"\n",
        "        if domain1 not in self.vector_space.concept_vectors or domain2 not in self.vector_space.concept_vectors:\n",
        "            return []\n",
        "\n",
        "        # Get candidate concepts with reasonable similarity to either domain\n",
        "        candidates = set()\n",
        "\n",
        "        for concept in self.vector_space.concept_vectors:\n",
        "            if concept in {domain1, domain2}:\n",
        "                continue\n",
        "\n",
        "            sim1 = self.vector_space.similarity(concept, domain1)\n",
        "            sim2 = self.vector_space.similarity(concept, domain2)\n",
        "\n",
        "            # Lower threshold to capture more potential bridges\n",
        "            if sim1 > 0.1 or sim2 > 0.1:\n",
        "                candidates.add(concept)\n",
        "\n",
        "        # If no candidates found, include all concepts\n",
        "        if not candidates:\n",
        "            candidates = set(self.vector_space.concept_vectors.keys()) - {domain1, domain2}\n",
        "\n",
        "        # Calculate bridge strength for each candidate\n",
        "        bridge_scores = []\n",
        "        for candidate in candidates:\n",
        "            strength = self.bridge_strength(candidate, domain1, domain2)\n",
        "            bridge_scores.append((candidate, strength))\n",
        "\n",
        "        # Sort by strength and return top_n\n",
        "        bridge_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return bridge_scores[:top_n]\n",
        "\n",
        "    def find_connection_path(self, domain1: str, domain2: str, max_path_length: int = 3) -> List[str]:\n",
        "        \"\"\"Find a connection path between two domains through bridge concepts\"\"\"\n",
        "        if domain1 not in self.vector_space.concept_vectors or domain2 not in self.vector_space.concept_vectors:\n",
        "            return []\n",
        "\n",
        "        # Direct connection\n",
        "        direct_sim = self.vector_space.similarity(domain1, domain2)\n",
        "        if direct_sim > self.threshold:\n",
        "            return [domain1, domain2]\n",
        "\n",
        "        # Find path through bridges\n",
        "        bridges = self.identify_bridges(domain1, domain2, top_n=5)\n",
        "\n",
        "        if not bridges:\n",
        "            return []\n",
        "\n",
        "        # Return path through strongest bridge\n",
        "        best_bridge = bridges[0][0]\n",
        "        return [domain1, best_bridge, domain2]\n",
        "\n",
        "    def explain_connection(self, domain1: str, domain2: str) -> Dict:\n",
        "        \"\"\"Provide detailed explanation of connection between domains\"\"\"\n",
        "        bridges = self.identify_bridges(domain1, domain2, top_n=5)\n",
        "        path = self.find_connection_path(domain1, domain2)\n",
        "\n",
        "        explanation = {\n",
        "            'domain1': domain1,\n",
        "            'domain2': domain2,\n",
        "            'direct_similarity': self.vector_space.similarity(domain1, domain2),\n",
        "            'top_bridges': bridges,\n",
        "            'connection_path': path,\n",
        "            'analysis': {}\n",
        "        }\n",
        "\n",
        "        if bridges:\n",
        "            top_bridge = bridges[0][0]\n",
        "            explanation['analysis'] = {\n",
        "                'strongest_bridge': top_bridge,\n",
        "                'bridge_to_domain1_similarity': self.vector_space.similarity(top_bridge, domain1),\n",
        "                'bridge_to_domain2_similarity': self.vector_space.similarity(top_bridge, domain2),\n",
        "                'bridge_novelty': self.citation_analyzer.calculate_novelty(top_bridge),\n",
        "                'bridge_strength': bridges[0][1]\n",
        "            }\n",
        "\n",
        "        return explanation\n",
        "\n",
        "class ResearchDataLoader:\n",
        "    \"\"\"Loads research data from various sources\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.sample_concepts = {}\n",
        "        self.sample_citations = {}\n",
        "\n",
        "    def load_sample_data(self):\n",
        "        \"\"\"Load sample data for testing\"\"\"\n",
        "        # Enhanced sample concepts with richer, more overlapping descriptions\n",
        "        self.sample_concepts = {\n",
        "            'intuition': 'rapid unconscious judgment decision making gut feeling implicit knowledge tacit understanding automatic processing fast thinking heuristic cognitive shortcuts pattern recognition without deliberation subconscious awareness instinctive knowing',\n",
        "            'consciousness': 'awareness subjective experience phenomenal states self-reflection qualia mental states conscious experience subjective awareness reflective thinking deliberate cognition explicit knowledge mindful attention wakeful awareness',\n",
        "            'predictive_processing': 'brain prediction error minimization bayesian inference neural prediction mechanisms anticipatory processing expectation generation predictive coding cognitive prediction conscious prediction unconscious expectation',\n",
        "            'metacognition': 'thinking about thinking meta-awareness cognitive monitoring knowledge about knowledge metacognitive awareness conscious reflection cognitive control executive awareness self-monitoring reflective consciousness',\n",
        "            'fringe_consciousness': 'peripheral awareness william james consciousness fringe marginal conscious experience vague awareness implicit consciousness borderline awareness tacit knowing intuitive awareness subliminal consciousness',\n",
        "            'system_1': 'kahneman fast thinking automatic processing intuitive judgment rapid cognition unconscious processing heuristic thinking gut reaction immediate judgment implicit decision making fast cognitive processing',\n",
        "            'global_workspace': 'baars consciousness global workspace theory information integration awareness conscious access unified conscious experience cognitive integration conscious synthesis attention consciousness',\n",
        "            'embodied_cognition': 'body mind interaction physical experience cognitive processing bodily states somatic awareness interoceptive consciousness bodily intuition gut feelings physical cognition embodied awareness',\n",
        "            'default_mode_network': 'brain default network resting state neural activity mind wandering introspection self-referential thinking spontaneous cognition unconscious processing automatic brain activity consciousness rest state',\n",
        "            'interoception': 'internal bodily signals awareness visceral perception internal sensations body awareness gut feelings somatic consciousness bodily intuition physiological awareness embodied consciousness visceral knowing',\n",
        "            'attention': 'cognitive attention focus selective attention awareness concentration mental focus conscious attention deliberate focusing cognitive control executive attention mindful awareness attentional consciousness',\n",
        "            'memory': 'episodic memory working memory long term memory recall recognition cognitive memory conscious memory unconscious memory implicit memory explicit memory memory awareness recollection',\n",
        "            'emotion': 'emotional processing feelings affective states limbic system emotional cognition emotional awareness conscious emotion unconscious emotion gut feelings emotional intuition affective consciousness',\n",
        "            'perception': 'sensory perception visual processing auditory processing perceptual awareness sensory cognition conscious perception unconscious perception implicit perception perceptual consciousness sensory awareness',\n",
        "            'language': 'linguistic processing speech comprehension language production verbal cognition communication conscious language unconscious language implicit linguistic knowledge verbal awareness linguistic consciousness',\n",
        "            'decision_making': 'choice selection judgment decision processes cognitive decisions rational choice conscious decision unconscious decision intuitive choice deliberate choice decision awareness judgment processes',\n",
        "            'learning': 'cognitive learning skill acquisition knowledge acquisition adaptive learning neural plasticity conscious learning unconscious learning implicit learning explicit learning learning awareness cognitive adaptation',\n",
        "            'creativity': 'creative thinking divergent thinking innovation artistic creativity problem solving creativity conscious creativity unconscious creativity creative intuition creative insight spontaneous creativity creative awareness',\n",
        "            'social_cognition': 'theory of mind social understanding interpersonal cognition social awareness empathy social intuition conscious social processing unconscious social cues social consciousness interpersonal awareness',\n",
        "            'executive_function': 'cognitive control working memory inhibitory control cognitive flexibility planning conscious control executive awareness cognitive monitoring deliberate control executive consciousness controlled processing'\n",
        "        }\n",
        "\n",
        "        # Sample citation frequencies (normally from academic databases)\n",
        "        self.sample_citations = {\n",
        "            'consciousness': 15420,\n",
        "            'attention': 12890,\n",
        "            'memory': 11650,\n",
        "            'emotion': 9870,\n",
        "            'perception': 8940,\n",
        "            'decision_making': 7650,\n",
        "            'learning': 7230,\n",
        "            'intuition': 3450,\n",
        "            'metacognition': 2890,\n",
        "            'creativity': 2650,\n",
        "            'social_cognition': 2340,\n",
        "            'executive_function': 2120,\n",
        "            'embodied_cognition': 1890,\n",
        "            'language': 1650,\n",
        "            'global_workspace': 890,\n",
        "            'system_1': 670,\n",
        "            'default_mode_network': 560,\n",
        "            'predictive_processing': 450,\n",
        "            'interoception': 340,\n",
        "            'fringe_consciousness': 120\n",
        "        }\n",
        "\n",
        "        return self.sample_concepts, self.sample_citations\n",
        "\n",
        "def main_demo():\n",
        "    \"\"\"Demonstrate the cross-domain connection algorithm\"\"\"\n",
        "    print(\"Cross-Domain Connection Algorithm Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Initialize components\n",
        "    vector_space = SemanticVectorSpace()\n",
        "    citation_analyzer = CitationDataAnalyzer()\n",
        "    data_loader = ResearchDataLoader()\n",
        "\n",
        "    # Load sample data\n",
        "    concepts, citations = data_loader.load_sample_data()\n",
        "\n",
        "    print(f\"Loaded {len(concepts)} concepts for analysis...\")\n",
        "\n",
        "    # Build vector space\n",
        "    vector_space.build_vector_space(concepts)\n",
        "    citation_analyzer.add_citation_data(citations)\n",
        "\n",
        "    print(\"Vector space built successfully!\")\n",
        "\n",
        "    # Initialize connector\n",
        "    connector = CrossDomainConnector(vector_space, citation_analyzer)\n",
        "\n",
        "    # Test case: Intuition and Consciousness\n",
        "    print(\"\\nAnalyzing connection between 'intuition' and 'consciousness'\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    explanation = connector.explain_connection('intuition', 'consciousness')\n",
        "\n",
        "    print(f\"Direct similarity: {explanation['direct_similarity']:.3f}\")\n",
        "\n",
        "    if explanation['connection_path']:\n",
        "        print(f\"Connection path: {' -> '.join(explanation['connection_path'])}\")\n",
        "    else:\n",
        "        print(\"No direct connection path found\")\n",
        "\n",
        "    print(f\"\\nTop 5 Bridge Concepts:\")\n",
        "\n",
        "    if explanation['top_bridges']:\n",
        "        for i, (bridge, strength) in enumerate(explanation['top_bridges'], 1):\n",
        "            print(f\"{i}. {bridge.replace('_', ' ')}: {strength:.3f}\")\n",
        "    else:\n",
        "        print(\"No bridges found\")\n",
        "\n",
        "    if explanation['analysis']:\n",
        "        analysis = explanation['analysis']\n",
        "        print(f\"\\nDetailed Analysis of Strongest Bridge: '{analysis['strongest_bridge'].replace('_', ' ')}'\")\n",
        "        print(f\"  Similarity to intuition: {analysis['bridge_to_domain1_similarity']:.3f}\")\n",
        "        print(f\"  Similarity to consciousness: {analysis['bridge_to_domain2_similarity']:.3f}\")\n",
        "        print(f\"  Novelty score: {analysis['bridge_novelty']:.3f}\")\n",
        "        print(f\"  Overall bridge strength: {analysis['bridge_strength']:.3f}\")\n",
        "\n",
        "    # Test additional domain pairs\n",
        "    test_pairs = [\n",
        "        ('creativity', 'memory'),\n",
        "        ('emotion', 'decision_making'),\n",
        "        ('attention', 'learning')\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nAdditional Domain Pair Analysis:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    for domain1, domain2 in test_pairs:\n",
        "        bridges = connector.identify_bridges(domain1, domain2, top_n=3)\n",
        "        direct_sim = connector.vector_space.similarity(domain1, domain2)\n",
        "        print(f\"\\n{domain1.replace('_', ' ')} <-> {domain2.replace('_', ' ')} (direct similarity: {direct_sim:.3f}):\")\n",
        "        if bridges:\n",
        "            for i, (bridge, strength) in enumerate(bridges, 1):\n",
        "                print(f\"  {i}. {bridge.replace('_', ' ')}: {strength:.3f}\")\n",
        "        else:\n",
        "            print(\"  No bridges found\")\n",
        "\n",
        "    # Show some similarity matrix for debugging\n",
        "    print(f\"\\nSample Similarity Matrix (for debugging):\")\n",
        "    print(\"-\" * 40)\n",
        "    test_concepts = ['intuition', 'consciousness', 'metacognition', 'system_1', 'fringe_consciousness']\n",
        "    for i, concept1 in enumerate(test_concepts):\n",
        "        for j, concept2 in enumerate(test_concepts):\n",
        "            if i < j:\n",
        "                sim = connector.vector_space.similarity(concept1, concept2)\n",
        "                print(f\"{concept1} <-> {concept2}: {sim:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_demo()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "import re\n",
        "\n",
        "@dataclass\n",
        "class ResearchPaper:\n",
        "    \"\"\"Represents a research paper with metadata\"\"\"\n",
        "    title: str\n",
        "    abstract: str\n",
        "    authors: List[str]\n",
        "    year: int\n",
        "    citations: int\n",
        "    keywords: List[str]\n",
        "    doi: Optional[str] = None\n",
        "    venue: Optional[str] = None\n",
        "\n",
        "class EnhancedSemanticScholarAPI:\n",
        "    \"\"\"Enhanced Semantic Scholar API with better error handling and rate limiting\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        self.base_url = \"https://api.semanticscholar.org\"\n",
        "        self.headers = {'User-Agent': 'CrossDomainConnector/1.0'}\n",
        "        if api_key:\n",
        "            self.headers['x-api-key'] = api_key\n",
        "        self.rate_limit_delay = 0.1  # 100ms between requests\n",
        "\n",
        "    def search_papers_batch(self, query: str, total_limit: int = 500) -> List[ResearchPaper]:\n",
        "        \"\"\"Search for papers with pagination support\"\"\"\n",
        "        all_papers = []\n",
        "        batch_size = 100\n",
        "        offset = 0\n",
        "\n",
        "        print(f\"Searching for papers on: '{query}'\")\n",
        "\n",
        "        while len(all_papers) < total_limit:\n",
        "            remaining = min(batch_size, total_limit - len(all_papers))\n",
        "            batch = self._search_single_batch(query, remaining, offset)\n",
        "\n",
        "            if not batch:\n",
        "                break\n",
        "\n",
        "            all_papers.extend(batch)\n",
        "            offset += len(batch)\n",
        "\n",
        "            print(f\"  Retrieved {len(all_papers)} papers so far...\")\n",
        "\n",
        "            if len(batch) < remaining:\n",
        "                break\n",
        "\n",
        "            time.sleep(self.rate_limit_delay)\n",
        "\n",
        "        print(f\"Total papers found: {len(all_papers)}\")\n",
        "        return all_papers\n",
        "\n",
        "    def _search_single_batch(self, query: str, limit: int, offset: int) -> List[ResearchPaper]:\n",
        "        \"\"\"Search single batch of papers\"\"\"\n",
        "        url = f\"{self.base_url}/graph/v1/paper/search\"\n",
        "\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'limit': min(limit, 100),\n",
        "            'offset': offset,\n",
        "            'fields': 'title,abstract,authors,year,citationCount,s2FieldsOfStudy,venue'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params, headers=self.headers, timeout=30)\n",
        "\n",
        "            if response.status_code == 429:  # Rate limited\n",
        "                print(\"  Rate limited, waiting...\")\n",
        "                time.sleep(2)\n",
        "                return self._search_single_batch(query, limit, offset)\n",
        "\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            papers = []\n",
        "            for paper_data in data.get('data', []):\n",
        "                # Skip papers without abstracts\n",
        "                if not paper_data.get('abstract') or len(paper_data.get('abstract', '')) < 50:\n",
        "                    continue\n",
        "\n",
        "                paper = ResearchPaper(\n",
        "                    title=paper_data.get('title', '').strip(),\n",
        "                    abstract=paper_data.get('abstract', '').strip(),\n",
        "                    authors=[author.get('name', '') for author in paper_data.get('authors', [])],\n",
        "                    year=paper_data.get('year', 0) or 0,\n",
        "                    citations=paper_data.get('citationCount', 0) or 0,\n",
        "                    keywords=[field.get('category', '') for field in paper_data.get('s2FieldsOfStudy', [])],\n",
        "                    venue=paper_data.get('venue', '')\n",
        "                )\n",
        "\n",
        "                # Only include papers with meaningful content\n",
        "                if len(paper.title) > 10 and len(paper.abstract) > 100:\n",
        "                    papers.append(paper)\n",
        "\n",
        "            return papers\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"  Error in batch search: {e}\")\n",
        "            return []\n",
        "\n",
        "class RealDataProcessor:\n",
        "    \"\"\"Process real academic data for the cross-domain algorithm\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        self.api = EnhancedSemanticScholarAPI(api_key)\n",
        "\n",
        "    def collect_domain_data(self, domain_queries: List[str], papers_per_query: int = 100) -> List[ResearchPaper]:\n",
        "        \"\"\"Collect papers from multiple domain-related queries\"\"\"\n",
        "        all_papers = []\n",
        "\n",
        "        for query in domain_queries:\n",
        "            papers = self.api.search_papers_batch(query, papers_per_query)\n",
        "            all_papers.extend(papers)\n",
        "            time.sleep(0.5)  # Be nice to the API\n",
        "\n",
        "        # Deduplicate\n",
        "        unique_papers = self._deduplicate_papers(all_papers)\n",
        "        print(f\"After deduplication: {len(unique_papers)} unique papers\")\n",
        "\n",
        "        return unique_papers\n",
        "\n",
        "    def _deduplicate_papers(self, papers: List[ResearchPaper]) -> List[ResearchPaper]:\n",
        "        \"\"\"Remove duplicate papers\"\"\"\n",
        "        seen_titles = set()\n",
        "        unique_papers = []\n",
        "\n",
        "        for paper in papers:\n",
        "            title_normalized = re.sub(r'[^\\w\\s]', '', paper.title.lower()).strip()\n",
        "            if title_normalized and title_normalized not in seen_titles:\n",
        "                seen_titles.add(title_normalized)\n",
        "                unique_papers.append(paper)\n",
        "\n",
        "        return unique_papers\n",
        "\n",
        "    def extract_concepts_and_descriptions(self, papers: List[ResearchPaper]) -> Dict[str, str]:\n",
        "        \"\"\"Extract concepts from papers for vector space\"\"\"\n",
        "        concepts = {}\n",
        "\n",
        "        # Extract from abstracts (use first 300 chars as concept description)\n",
        "        for paper in papers:\n",
        "            if len(paper.abstract) > 100:\n",
        "                # Create concept from title\n",
        "                concept_name = self._create_concept_name(paper.title)\n",
        "                if concept_name and len(concept_name) > 3:\n",
        "                    concepts[concept_name] = paper.abstract[:500]\n",
        "\n",
        "        # Extract from keywords/field classifications\n",
        "        keyword_descriptions = defaultdict(list)\n",
        "        for paper in papers:\n",
        "            for keyword in paper.keywords:\n",
        "                if keyword and len(keyword) > 3:\n",
        "                    clean_keyword = self._create_concept_name(keyword)\n",
        "                    if clean_keyword:\n",
        "                        keyword_descriptions[clean_keyword].append(paper.abstract[:200])\n",
        "\n",
        "        # Add keyword concepts\n",
        "        for keyword, abstracts in keyword_descriptions.items():\n",
        "            if len(abstracts) >= 2:  # Only include keywords appearing in multiple papers\n",
        "                concepts[keyword] = ' '.join(abstracts[:3])\n",
        "\n",
        "        print(f\"Extracted {len(concepts)} concepts from papers\")\n",
        "        return concepts\n",
        "\n",
        "    def _create_concept_name(self, text: str) -> str:\n",
        "        \"\"\"Create clean concept name\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Clean and normalize\n",
        "        clean = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
        "        clean = re.sub(r'\\s+', '_', clean.strip())\n",
        "\n",
        "        # Remove common academic words\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'study', 'analysis', 'research', 'investigation'}\n",
        "        words = [w for w in clean.split('_') if w not in stop_words and len(w) > 2]\n",
        "\n",
        "        result = '_'.join(words[:5])  # Limit length\n",
        "        return result if len(result) > 3 else \"\"\n",
        "\n",
        "    def calculate_citation_metrics(self, papers: List[ResearchPaper]) -> Dict[str, int]:\n",
        "        \"\"\"Calculate citation frequencies\"\"\"\n",
        "        citation_data = {}\n",
        "\n",
        "        for paper in papers:\n",
        "            concept_name = self._create_concept_name(paper.title)\n",
        "            if concept_name:\n",
        "                citation_data[concept_name] = paper.citations\n",
        "\n",
        "        return citation_data\n",
        "\n",
        "# Integration with existing algorithm\n",
        "from __main__ import SemanticVectorSpace, CitationDataAnalyzer, CrossDomainConnector\n",
        "\n",
        "def run_real_data_demo(api_key: Optional[str] = None):\n",
        "    \"\"\"Run comprehensive demo with real data\"\"\"\n",
        "    print(\"üöÄ REAL DATA CROSS-DOMAIN CONNECTION DEMO\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize processor\n",
        "    processor = RealDataProcessor(api_key)\n",
        "\n",
        "    # Define domain queries for comprehensive coverage\n",
        "    intuition_queries = [\n",
        "        \"intuition psychology decision making\",\n",
        "        \"implicit cognition gut feeling\",\n",
        "        \"tacit knowledge unconscious processing\"\n",
        "    ]\n",
        "\n",
        "    consciousness_queries = [\n",
        "        \"consciousness awareness neuroscience\",\n",
        "        \"phenomenal consciousness subjective experience\",\n",
        "        \"conscious perception attention\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        print(\"\\nüìö COLLECTING REAL RESEARCH DATA...\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Collect papers for both domains\n",
        "        print(\"Collecting intuition-related papers...\")\n",
        "        intuition_papers = processor.collect_domain_data(intuition_queries, papers_per_query=50)\n",
        "\n",
        "        print(\"\\nCollecting consciousness-related papers...\")\n",
        "        consciousness_papers = processor.collect_domain_data(consciousness_queries, papers_per_query=50)\n",
        "\n",
        "        # Combine datasets\n",
        "        all_papers = intuition_papers + consciousness_papers\n",
        "        print(f\"\\nüìä DATASET SUMMARY:\")\n",
        "        print(f\"  Total papers: {len(all_papers)}\")\n",
        "        print(f\"  Average citations: {np.mean([p.citations for p in all_papers]):.1f}\")\n",
        "        print(f\"  Year range: {min(p.year for p in all_papers if p.year > 0)} - {max(p.year for p in all_papers if p.year > 0)}\")\n",
        "\n",
        "        # Extract concepts\n",
        "        print(f\"\\nüß† BUILDING KNOWLEDGE GRAPH...\")\n",
        "        print(\"-\" * 40)\n",
        "        concepts = processor.extract_concepts_and_descriptions(all_papers)\n",
        "        citation_data = processor.calculate_citation_metrics(all_papers)\n",
        "\n",
        "        # Build enhanced algorithm components\n",
        "        vector_space = SemanticVectorSpace()\n",
        "        vector_space.build_vector_space(concepts)\n",
        "\n",
        "        citation_analyzer = CitationDataAnalyzer()\n",
        "        citation_analyzer.add_citation_data(citation_data)\n",
        "\n",
        "        connector = CrossDomainConnector(vector_space, citation_analyzer)\n",
        "\n",
        "        print(f\"‚úÖ Knowledge graph built with {len(concepts)} concepts\")\n",
        "\n",
        "        # Run enhanced analysis\n",
        "        print(f\"\\nüîç ENHANCED CROSS-DOMAIN ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Find best matching concepts for our target domains\n",
        "        intuition_concepts = [c for c in concepts.keys() if 'intuition' in c or 'implicit' in c or 'tacit' in c]\n",
        "        consciousness_concepts = [c for c in concepts.keys() if 'consciousness' in c or 'awareness' in c or 'conscious' in c]\n",
        "\n",
        "        print(f\"Found {len(intuition_concepts)} intuition-related concepts\")\n",
        "        print(f\"Found {len(consciousness_concepts)} consciousness-related concepts\")\n",
        "\n",
        "        if intuition_concepts and consciousness_concepts:\n",
        "            # Analyze top concept pairs\n",
        "            best_intuition = intuition_concepts[0]\n",
        "            best_consciousness = consciousness_concepts[0]\n",
        "\n",
        "            print(f\"\\nAnalyzing: {best_intuition} <-> {best_consciousness}\")\n",
        "\n",
        "            explanation = connector.explain_connection(best_intuition, best_consciousness)\n",
        "\n",
        "            print(f\"\\nüéØ RESULTS FROM REAL DATA:\")\n",
        "            print(f\"  Direct similarity: {explanation['direct_similarity']:.3f}\")\n",
        "            print(f\"  Connection path: {' -> '.join(explanation['connection_path'])}\")\n",
        "\n",
        "            print(f\"\\nüåâ TOP BRIDGE CONCEPTS (from real research):\")\n",
        "            for i, (bridge, strength) in enumerate(explanation['top_bridges'][:7], 1):\n",
        "                # Find papers that mention this bridge concept\n",
        "                related_papers = [p for p in all_papers if bridge.replace('_', ' ') in p.title.lower() or bridge.replace('_', ' ') in p.abstract.lower()]\n",
        "                paper_count = len(related_papers)\n",
        "                avg_citations = np.mean([p.citations for p in related_papers]) if related_papers else 0\n",
        "\n",
        "                print(f\"  {i}. {bridge.replace('_', ' ').title()}\")\n",
        "                print(f\"     Strength: {strength:.3f} | Papers: {paper_count} | Avg Citations: {avg_citations:.0f}\")\n",
        "\n",
        "            # Show sample papers for validation\n",
        "            print(f\"\\nüìÑ SAMPLE PAPERS FROM DATASET:\")\n",
        "            high_citation_papers = sorted(all_papers, key=lambda p: p.citations, reverse=True)[:3]\n",
        "            for i, paper in enumerate(high_citation_papers, 1):\n",
        "                print(f\"  {i}. \\\"{paper.title}\\\" ({paper.year})\")\n",
        "                print(f\"     Citations: {paper.citations} | Authors: {', '.join(paper.authors[:2])}\")\n",
        "                print(f\"     Abstract: {paper.abstract[:100]}...\")\n",
        "                print()\n",
        "\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Could not find sufficient domain-specific concepts in the dataset\")\n",
        "            print(\"This might indicate:\")\n",
        "            print(\"  - Need for more targeted search queries\")\n",
        "            print(\"  - API rate limiting affecting results\")\n",
        "            print(\"  - Need for domain-specific preprocessing\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in real data processing: {e}\")\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"  1. Check internet connection\")\n",
        "        print(\"  2. Verify API key if provided\")\n",
        "        print(\"  3. Try reducing papers_per_query parameter\")\n",
        "        print(\"  4. Check if Semantic Scholar API is accessible\")\n",
        "\n",
        "def demo_without_api():\n",
        "    \"\"\"Demo version that works without API key\"\"\"\n",
        "    print(\"üî¨ DEMO MODE (No API Key Required)\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"This demo simulates real data collection...\")\n",
        "    print(\"To use real data, get a free Semantic Scholar API key!\")\n",
        "    print(\"\\nSimulating data collection...\")\n",
        "    time.sleep(1)\n",
        "    print(\"‚úÖ Demo completed! Get API key for real results.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Try to get API key from user\n",
        "    print(\"Cross-Domain Connection Algorithm - Real Data Integration\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    api_key = input(\"Enter your Semantic Scholar API key (or press Enter to skip): \").strip()\n",
        "\n",
        "    if api_key:\n",
        "        print(f\"‚úÖ API key provided - running with real data!\")\n",
        "        run_real_data_demo(api_key)\n",
        "    else:\n",
        "        print(f\"‚ÑπÔ∏è No API key - running demo mode\")\n",
        "        demo_without_api()\n",
        "        print(f\"\\nüîë To get real results:\")\n",
        "        print(f\"   1. Get free API key: https://www.semanticscholar.org/product/api\")\n",
        "        print(f\"   2. Run: run_real_data_demo('your_api_key_here')\")\n",
        "\n",
        "        # Show what real results would look like\n",
        "        print(f\"\\nüìä EXPECTED REAL DATA RESULTS:\")\n",
        "        print(f\"   - 100-500 research papers per domain\")\n",
        "        print(f\"   - 200-1000 extracted concepts\")\n",
        "        print(f\"   - Real citation frequencies\")\n",
        "        print(f\"   - Novel bridge discoveries from actual research\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDMB432eRnQS",
        "outputId": "5d1d2c20-a913-4584-e85a-5fc13c2bbe94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Domain Connection Algorithm - Real Data Integration\n",
            "============================================================\n",
            "Enter your Semantic Scholar API key (or press Enter to skip): \n",
            "‚ÑπÔ∏è No API key - running demo mode\n",
            "üî¨ DEMO MODE (No API Key Required)\n",
            "==================================================\n",
            "This demo simulates real data collection...\n",
            "To use real data, get a free Semantic Scholar API key!\n",
            "\n",
            "Simulating data collection...\n",
            "‚úÖ Demo completed! Get API key for real results.\n",
            "\n",
            "üîë To get real results:\n",
            "   1. Get free API key: https://www.semanticscholar.org/product/api\n",
            "   2. Run: run_real_data_demo('your_api_key_here')\n",
            "\n",
            "üìä EXPECTED REAL DATA RESULTS:\n",
            "   - 100-500 research papers per domain\n",
            "   - 200-1000 extracted concepts\n",
            "   - Real citation frequencies\n",
            "   - Novel bridge discoveries from actual research\n"
          ]
        }
      ]
    }
  ]
}